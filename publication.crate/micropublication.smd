```python exec
# This code makes the micropublication aware of its interface data.
from rocrate.rocrate import ROCrate
from dataclasses import dataclass
from datetime import datetime
from typing import Optional
from pathlib import Path

import pandas as pd
import numpy as np
import requests
import json
import os
import re

# Load the transect data
data_path = os.path.join(Path.cwd(), "data.json")
df = pd.read_json(data_path, typ='series')

def round_if_number(val):
    if isinstance(val, (float, int)):
        return round(val, 2)
    return val

@dataclass
class SiteData:
    id: str
    site_id: str
    orientation: float
    along_dist: float
    along_dist_norm: float
    beach_slope: float
    cil: float
    ciu: float
    trend: float
    n_points: float
    n_points_nonan: float
    r2_score: float
    mae: float
    mse: float
    rmse: float
    intercept: float
    ERODIBILITY: Optional[float]

site_data = SiteData(**{k: round_if_number(v) for k, v in df.to_dict().items()})
```

```python exec 
# Configuration Variables
min_points_for_valid_regression = 10 # Minimum number of non-NaN points for the linear regression to be considered plausible
trend_threshold = 0.01 # Threshold for trend value to be considered significant -- i.e. is the slope big enough to care about. 
# Look into using the standard deviation of the trend values to set a noise threshold
noise_threshold = float(round(2 * abs(site_data.trend) * np.sqrt(site_data.n_points_nonan), 3)) # Even if the trend is big enough, is the signal drowned by noise?
show_trend = True # We only show the trend if it is interpreted as valid 
weak_trend_threshold = 0.05 # Threshold for weak trend, in m/year
strong_trend_threshold = 0.1 # Threshold for strong trend, in m/year

zero_r2_threshold = 0 # R² score below which the regression line fits worse than a flat model
low_r2_threshold = 0.3 # R² score below which the trend fit is only moderately valid
moderate_r2_threshold = 0.3 # R² score below which the trend fit is only moderately valid
high_r2_threshold = 0.8 # R² score indicating a strong trend fit
one_r2_theshold = 1 # R² score indicating a perfect fit
```

```python exec always
# This block uses the coordinates to reverse geocode and get the location name
coordinates_path = os.path.join(Path.cwd(), "coordinates.json")
lon, lat = pd.read_json(coordinates_path, typ='series').to_dict()['coordinates'][0]
location = requests.get(
    url="https://nominatim.openstreetmap.org/reverse",
    params={
        'lat': lat,
        'lon': lon,
        'format': 'json',
        'zoom': 10,
        'addressdetails': 1
    },
    headers={'User-Agent': 'CoastSat-Micropublication'}
).json().get('display_name')
```

```python exec always
# Load the publication.crate, interface.crate, and batch_processes.crate manifest files
publication_crate_path = Path.cwd() 
interface_crate_path = Path.cwd() / "interface.crate" 
batch_processes_crate_path = interface_crate_path / "batch_processes"

try:
    if publication_crate_path.exists() and interface_crate_path.exists() and batch_processes_crate_path.exists():
        publication_crate = ROCrate(publication_crate_path)
        interface_crate = ROCrate(interface_crate_path)
        batch_processes_crate = ROCrate(batch_processes_crate_path)
except Exception as e:
    publication_crate = None
    interface_crate = None
    print(f"Error loading publication crate: {e}")
```

```python exec 
# Before narrative content, some helper functions to interface with the crates:
def query_by_link(crate, prop, target_id, match_substring=False):
    """
    Return entities (dict or ContextEntity) whose `prop` links to `target_id`.
    If `match_substring` is True, will return entities whose link includes `target_id` as a substring.
    """
    is_rocrate = hasattr(crate, "get_entities")
    entities = crate.get_entities() if is_rocrate else crate.get("@graph", [])
    out = []

    for e in entities:
        val = (e.properties().get(prop) if is_rocrate else e.get(prop))
        if val is None:
            continue
        vals = [val] if not isinstance(val, list) else val

        ids = [
            (x.id if hasattr(x, "id") else x.get("@id") if isinstance(x, dict) else x)
            for x in vals
        ]
        if match_substring:
            if any(target_id in _id for _id in ids if isinstance(_id, str)):
                out.append(e)
        else:
            if target_id in ids:
                out.append(e)
    return out

def filter_linked_entities_by_substring(crate, entities, prop, substring):
    """
    For a given list of entities, follow `prop` links (e.g., 'object') and return
    all linked entities whose `@id` includes `substring`.
    """
    is_rocrate = hasattr(crate, "get_entities")
    all_entities = crate.get_entities() if is_rocrate else crate.get("@graph", [])

    # Index entities by ID for fast lookup
    id_index = {
        (e.id if hasattr(e, "id") else e.get("@id")): e
        for e in all_entities
    }
    matched = []
    for entity in entities:
        val = entity.properties().get(prop) if is_rocrate else entity.get(prop)
        if val is None:
            continue

        vals = [val] if not isinstance(val, list) else val

        for v in vals:
            target_id = (
                v.id if hasattr(v, "id") else v.get("@id") if isinstance(v, dict) else v
            )

            if not isinstance(target_id, str):
                continue

            if substring in target_id:
                linked_entity = id_index.get(target_id)
                if linked_entity:
                    matched.append(linked_entity)

    return matched

def resolve_linked_entity(crate, entity, prop):
    """
    Follow a single-valued property (like 'instrument') from an entity,
    and return the linked entity (resolved from the crate).
    Returns None if the property is missing or not resolvable.
    """
    is_rocrate = hasattr(crate, "get_entities")
    all_entities = crate.get_entities() if is_rocrate else crate.get("@graph", [])

    # Build a lookup of @id to entity
    id_index = {
        (e.id if hasattr(e, "id") else e.get("@id")): e
        for e in all_entities
    }

    val = entity.properties().get(prop) if is_rocrate else entity.get(prop)
    if val is None:
        return None

    # Normalize to string ID
    if isinstance(val, list):
        raise ValueError(f"Expected only one linked entity in property '{prop}', but found a list.")
    
    target_id = (
        val.id if hasattr(val, "id") else val.get("@id") if isinstance(val, dict) else val
    )

    return id_index.get(target_id)

# And now helper general helper functions for data transofmation and narrative generation
def convert_to_raw_url(github_url: str) -> str:
    """
    Converts a GitHub blob URL to a raw.githubusercontent URL.
    """
    match = re.match(r"https://github\.com/(.+)/blob/([a-f0-9]+)/(.+)", github_url)
    if not match:
        raise ValueError("Invalid GitHub blob URL format.")
    user_repo, commit_hash, path = match.groups()
    return f"https://raw.githubusercontent.com/{user_repo}/{commit_hash}/{path}"

def get_appended_rows(
    old_url: str,
    new_url: str,
    filter_column: str = None,
    filter_value: str = None
) -> pd.DataFrame:
    raw_old_url = convert_to_raw_url(old_url)
    raw_new_url = convert_to_raw_url(new_url)
    
    old_df = pd.read_csv(raw_old_url)
    new_df = pd.read_csv(raw_new_url)
    
    appended_df = pd.concat([new_df, old_df]).drop_duplicates(keep=False)

    # If you're looking for a particular transect column (e.g., 'sar0003-0003'),
    # this assumes the filter_column is in the wide format as a column name.
    if filter_column and filter_column in appended_df.columns:
        # Only keep rows where that transect column is not NaN
        appended_df = appended_df[~appended_df[filter_column].isna()]

    return appended_df
```

# **`site_data.id`{python exec}** Transect 
## `location`{python exec}

::: if site_data.n_points_nonan < min_points_for_valid_regression

> [!failure]- The trend fit is not valid due to insufficient data points.
> The number of non-NaN points used in the analysis is **`site_data.n_points_nonan`{python exec}**, which is below the minimum required of **{min_points_for_valid_regression}**.

```python exec
show_trend = False
```
:::

::: if (abs(site_data.trend) < trend_threshold) 

> [!failure]- The trend fit is not valid due to a trend value of **`site_data.trend`{python exec}** m/year.
> This trend is below the threshold of **{trend_threshold}** m/year.

```python exec
show_trend = False
```

:::

::: if (site_data.rmse > noise_threshold)

> [!failure]- Root Mean Square Error (RMSE) is too high.
> The trend fit is not valid because the root mean square error (RMSE) of **`site_data.rmse`{python exec}** is higher than the noise threshold of **`noise_threshold`{python exec}**, indicating that the variability in the data is high enough for the trend to be considered unreliable.

```python exec
show_trend = False
```
:::

::: if site_data.r2_score == zero_r2_threshold

> [!warning]- The linear regression accounts for none of the variance.
> This is likely due to a lack of data or a poor fit to the data. The R² score is **`site_data.r2_score`{python exec}**, which indicates that the model does not explain the variability in the data well.

```python exec
show_trend = False
```
::: elif site_data.r2_score <= low_r2_threshold

> [!warning]- The linear regression accounts for a low proportion of the variance.
> This is likely due to a lack of data or a poor fit to the data. The R² score is **`site_data.r2_score`{python exec}**, which indicates that the model does not explain the variability in the data well.

::: elif (site_data.r2_score <= moderate_r2_threshold)

> [!info]- The trend fit is only moderately valid.
> The R² score is **`site_data.r2_score`{python exec}**. This suggests that the model explains some variability in the data, but the fit is not strong.

::: elif site_data.r2_score <= high_r2_threshold

> [!info]- The trend fit has a high R² score.
> The trend fit is valid with a high R² score of **`site_data.r2_score`{python exec}**. This indicates that the linear regression model explains a large portion of the variability in the data, suggesting a strong trend.

::: elif site_data.r2_score == one_r2_theshold

> [!info]- The trend fit has an excellent R² score.
> This indicates that the linear regression model explains all the variability in the data, suggesting a very strong trend.
The trend fit is valid with an excellent R² score of **`site_data.r2_score`{python exec}**. This indicates that the linear regression model explains almost all the variability in the data, suggesting a very strong trend.

:::


::: if show_trend

:::: if abs(site_data.trend) < weak_trend_threshold

::::: if site_data.trend < 0

> [!tip]-
> The coast is predicted to erode very slightly, with a trend of **`site_data.trend`{python exec}** m/year. This trend is considered negligible.

::::: else

> [!tip]-
> The coast is predicted to accrete very slightly, with a trend of **`site_data.trend`{python exec}** m/year. This trend is considered negligible.

:::::

:::: elif abs(site_data.trend) < strong_trend_threshold

::::: if site_data.trend < 0

> [!tip]-
> The coast is predicted to erode, with a trend of **`site_data.trend`{python exec}** m/year. This trend is considered weak.

::::: else

> [!tip]-
> The coast is predicted to accrete, with a trend of **`site_data.trend`{python exec}** m/year. This trend is considered weak.

:::::

:::: else

::::: if site_data.trend < 0

> [!tip]-
> The coast is predicted to erode significantly, with a trend of **`site_data.trend`{python exec}** m/year. This trend is considered strong.

::::: else

> [!tip]-
> The coast is predicted to accrete significantly, with a trend of **`site_data.trend`{python exec}** m/year. This trend is considered strong.

:::::

::::

:::

```python exec
# Retrieve urls to current versions of the primary result and per-site data
# These are used to link to the data files in the narrative content.
primary_result = query_by_link(interface_crate, "exampleOfWork", "#fp-transectsextended-3")[0]
per_site_data = [
    e for e in query_by_link(interface_crate, "exampleOfWork", "#fp-transect_site_xlsx-1")
    if (e.properties().get("name") if hasattr(e, "properties") else e.get("name")) == f"{site_data.site_id}.xlsx"
]
coastsat_git_url = interface_crate.mainEntity.get("version")
primary_result_url = primary_result.get("@id") if primary_result else None
per_site_data_url = per_site_data[0].get("@id") if per_site_data else None

# Get the publication.crate datePublished -- this represents the dat of the last commit and therefore the last processing of the data.
date_str = interface_crate.mainEntity.get("datePublished", "Unknown date")
try:
    run_date = datetime.fromisoformat(date_str).strftime("%B %d, %Y")
except Exception:
    run_date = date_str
```

::: if "nzd" in site_data.id 

```python exec
# For the growing NZ dataset, we want to link and compare the current version of the data with the previous version.
# First, load the create_action which models the batch processing of NZ transects
nz_create_action = batch_processes_crate.get("#batch-process-nz")
# Grab the instrument responsible for transforming the data
instrument = resolve_linked_entity(batch_processes_crate, nz_create_action, "instrument")

# Then, filter the linked entities to find the previous and current timeseries for this site.
previous_timeserries = filter_linked_entities_by_substring(batch_processes_crate, [nz_create_action], "object", site_data.site_id)
current_timeserries = filter_linked_entities_by_substring(batch_processes_crate, [nz_create_action], "result", site_data.site_id)

# We expect prev and current timeseries to be a single entity. Check, and if not, raise an error.
if len(previous_timeserries) != 1 or len(current_timeserries) != 1:
    raise ValueError(f"Expected exactly one previous and one current timeseries for site {site_data.site_id}, but found {len(previous_timeserries)} and {len(current_timeserries)} respectively.")

previous_timeserries = previous_timeserries[0]
current_timeserries = current_timeserries[0]

# Get names and urls for embedding into mermaid
previous_timeserries_name = previous_timeserries.get("name", "Previous Timeseries")
current_timeserries_name = current_timeserries.get("name", "Current Timeseries")
instrument_name = instrument.get("name", "Unknown Instrument")
previous_timeserries_url = previous_timeserries.get("@id")
current_timeserries_url = current_timeserries.get("@id")
instrument_url = instrument.get("codeRepository")
```

Data for this transect can be downloaded `dict(type="Link", target=per_site_data_url, content=[dict(type="Text", value=f"here")])`{python exec}, and linear model results for all transects can be downloaded `dict(type="Link", target=primary_result_url, content=[dict(type="Text", value=f"here")])`{python exec}.
The data used to inform the linear regression and trend analysis is drawn from a growing dataset, processed monthly. 

:::: if (previous_timeserries.get('sha256') == current_timeserries.get('sha256'))

Linear model results for all transects can be downloaded `dict(type="Link", target=primary_result_url, content=[dict(type="Text", value=f"here")])`{python exec}.
The data for this transect has not changed since the last processing. The current version of the data is identical to the previous version.

:::: else 

### Transect Data Characteristics

```python exec
# Logic to get the transect data characteristics
raw_new_url = convert_to_raw_url(current_timeserries_url)
df = pd.read_csv(raw_new_url)
date_min = pd.to_datetime(df["dates"]).min().strftime("%B %d, %Y")
date_max = pd.to_datetime(df["dates"]).max().strftime("%B %d, %Y")
```

::: if site_data.orientation is not None

Linear model results for all transects can be downloaded `dict(type="Link", target=primary_result_url, content=[dict(type="Text", value=f"here")])`{python exec}.
This transect contains **`site_data.n_points`{python exec}** data points, with **`site_data.n_points_nonan`{python exec}** no-NaN values. The data spans from `date_min`{python exec} to `date_max`{python exec}, and the transect is oriented at **`site_data.orientation`{python exec}** degrees.

::: else 

Linear model results for all transects can be downloaded `dict(type="Link", target=primary_result_url, content=[dict(type="Text", value=f"here")])`{python exec}.
This transect contains **`site_data.n_points`{python exec}** data points, with **`site_data.n_points_nonan`{python exec}** no-NaN values. The data spans from `date_min`{python exec} to `date_max`{python exec}.

:::

### Data Production


This transect has been updated on `run_date`{python exec} during the current processing cycle, and was processed using the instrument `instrument_name`{python exec}.
The graph below illustrates the data flow from the previous timeseries to the current timeseries, and provides links to (1) the previous and current timeseries data files, (2) the instrument used for processing, and (3) a link to a secondary publication.crate describing the CoastSat methodology.

```mermaid exec
flowchart LR
    subgraph Data-Preprocessing
        A[/"{{ previous_timeserries_name }}"/]-->B["{{ instrument_name }}"]
        B["{{ instrument_name }}"]-->C[/"{{ current_timeserries_name }}"/]
    end
    subgraph Analysis
        D[/"Trend Analysis"/]
    end

    Data-Preprocessing --> Analysis

    Analysis --> E[/"Transect Regression Data"/]

    click A href "{{ previous_timeserries_url }}" _blank
    click B href "{{ instrument_url }}" _blank
    click C href "{{ current_timeserries_url }}" _blank
    click E href "{{ primary_result_url }}" _blank
```

> [!info]+ Batch Process Instrumentation
> <div> <script src="https://gist.github.com/GusEllerm/b8c382728f321ae25c224a201bca476b.js"></script> </div>

> [!info]+ Data Appended to Transect
> The table below shows the appended data for this transect, described by the processes within the Data-Preprocessing subgraph in Figure X.
> ```python exec 
> # Only return rows that have data for the transect 
> df = get_appended_rows(previous_timeserries_url, current_timeserries_url, filter_column=site_data.id)
> df = df[["dates", site_data.id, "satname"]]
> df["dates"] = pd.to_datetime(df["dates"]).dt.strftime("%B %d, %Y")
> # Rename column to include units
> df = df.rename(columns={site_data.id: f"{site_data.id} (meters)"})
> df
> ```

::::

::: elif "sar" in site_data.id

```python exec
# For the growing Sardinia dataset, we want to link and compare the current version of the data with the previous version.
# First, load the create_action which models the batch processing of Sardinia transects
sar_create_action = batch_processes_crate.get("#batch-process-sardinia")
# Grab the instrument responsible for transforming the data
instrument = resolve_linked_entity(batch_processes_crate, sar_create_action, "instrument")
# Then, filter the linked entities to find the previous and current timeseries for this site.
previous_timeserries = filter_linked_entities_by_substring(batch_processes_crate, [sar_create_action], "object", site_data.site_id)
current_timeserries = filter_linked_entities_by_substring(batch_processes_crate, [sar_create_action], "result", site_data.site_id)

# We expect prev and current timeseries to be a single entity. Check, and if not, raise an error.
if len(previous_timeserries) != 1 or len(current_timeserries) != 1:
    raise ValueError(f"Expected exactly one previous and one current timeseries for site {site_data.site_id}, but found {len(previous_timeserries)} and {len(current_timeserries)} respectively.")

previous_timeserries = previous_timeserries[0]
current_timeserries = current_timeserries[0]

# Get names and urls for embedding into mermaid
previous_timeserries_name = previous_timeserries.get("name", "Previous Timeseries")
current_timeserries_name = current_timeserries.get("name", "Current Timeseries")
instrument_name = instrument.get("name", "Unknown Instrument")
previous_timeserries_url = previous_timeserries.get("@id")
current_timeserries_url = current_timeserries.get("@id")
instrument_url = instrument.get("codeRepository")
```

:::: if (previous_timeserries.get('sha256') == current_timeserries.get('sha256'))

Linear model results for all transects can be downloaded `dict(type="Link", target=primary_result_url, content=[dict(type="Text", value=f"here")])`{python exec}.
The data for this transect has not changed since the last processing. The current version of the data is identical to the previous version.

:::: else 

### Transect Data Characteristics

```python exec
# Logic to get the transect data characteristics
raw_new_url = convert_to_raw_url(current_timeserries_url)
df = pd.read_csv(raw_new_url)
date_min = pd.to_datetime(df["dates"]).min().strftime("%B %d, %Y")
date_max = pd.to_datetime(df["dates"]).max().strftime("%B %d, %Y")
```

::: if site_data.orientation is not None

Linear model results for all transects can be downloaded `dict(type="Link", target=primary_result_url, content=[dict(type="Text", value=f"here")])`{python exec}.
This transect contains **`site_data.n_points`{python exec}** data points, with **`site_data.n_points_nonan`{python exec}** no-NaN values. The data spans from `date_min`{python exec} to `date_max`{python exec}, and the transect is oriented at **`site_data.orientation`{python exec}** degrees.

::: else

Linear model results for all transects can be downloaded `dict(type="Link", target=primary_result_url, content=[dict(type="Text", value=f"here")])`{python exec}.
This transect contains **`site_data.n_points`{python exec}** data points, with **`site_data.n_points_nonan`{python exec}** no-NaN values. The data spans from `date_min`{python exec} to `date_max`{python exec}.

:::

### Data Production

This transect has been updated on `run_date`{python exec} during the current processing cycle, and was processed using the instrument `instrument_name`{python exec}.
The graph below illustrates the data flow from the previous timeseries to the current timeseries, and provides links to (1) the previous and current timeseries data files, (2) the instrument used for processing, and (3) a link to a secondary publication.crate describing the CoastSat methodology.

```mermaid exec
flowchart LR
    subgraph Data-Preprocessing
        A[/"{{ previous_timeserries_name }}"/]-->B["{{ instrument_name }}"]
        B["{{ instrument_name }}"]-->C[/"{{ current_timeserries_name }}"/]
    end
    subgraph Analysis
        D[/"Trend Analysis"/]
    end

    Data-Preprocessing --> Analysis

    Analysis --> E[/"Transect Regression Data"/]

    click A href "{{ previous_timeserries_url }}" _blank
    click B href "{{ instrument_url }}" _blank
    click C href "{{ current_timeserries_url }}" _blank
    click E href "{{ primary_result_url }}" _blank
```

> [!info]+ Data Appended to Transect
> The table below shows the appended data for this transect, described by the processes within the Data-Preprocessing subgraph in Figure X.
> ```python exec 
> # Only return rows that have data for the transect 
> df = get_appended_rows(previous_timeserries_url, current_timeserries_url, filter_column=site_data.id)
> df = df[["dates", site_data.id, "satname"]]
> df["dates"] = pd.to_datetime(df["dates"]).dt.strftime("%B %d, %Y")
> # Rename column to include units
> df = df.rename(columns={site_data.id: f"{site_data.id} (meters)"})
> df
> ```


::::

::: else

pacific rim

:::


#### Micropublication Data

```python exec
interface_crate_version = interface_crate.mainEntity.get("url")
coastsat_version = interface_crate.mainEntity.get("version")
publication_crate_version = publication_crate.mainEntity.get("url")
```

This micropublication is part of the CoastSat project, which aims to monitor coastal change using satellite imagery.
It is based on `dict(type="Link", target=interface_crate_version, content=[dict(type="Text", value=f"this")])`{python exec} version of the _interface.crate_, and `dict(type="Link", target=coastsat_version, content=[dict(type="Text", value=f"this")])`{python exec} version of the CoastSat GitHub repository.

> [!tip]- Micropublication publication.crate
> This publication.crate is avaliable for download `dict(type="Link", target=publication_crate_version, content=[dict(type="Text", value=f"here")])`{python exec}.
> It contains all data and resources required to reproduce the analysis and results presented in this micropublication.

### Threshold Values for Narrative Generation

The following tables summarize the threshold values used to determine the validity of the trend fit and the conditions under which the trend is shown in the narrative content.

> [!info]+ Threshold Values
> |        | Threshold         | Actual Value         | Description |
> |---------|-------------------|---------------------| -------------|
> | Zero R2 score | < `zero_r2_threshold`{python exec} | `site_data.r2_score`{python exec} | R² score below which the regression line fits worse than a flat model |
> | Low R2 score | < `low_r2_threshold`{python exec} | `site_data.r2_score`{python exec} | R² score below which the trend fit is not valid |
> | Moderate R2 score | < `moderate_r2_threshold`{python exec} | `site_data.r2_score`{python exec} | R² score below which the trend fit is only moderately valid |
> | High R2 score | < 0.8 | `site_data.r2_score`{python exec} | R² score indicating a strong trend fit |
> | Excellent R2 score | < `one_r2_theshold`{python exec} | `site_data.r2_score`{python exec} | R² score indicating a perfect fit |

> [!info]+ Trend and Noise Thresholds
> |        | Threshold         | Actual Value         | Description |
> |---------|-------------------|---------------------|-------------|
> | Minimum Points for Valid Regression | `min_points_for_valid_regression`{python exec} | `site_data.n_points_nonan`{python exec} | Minimum number of non-NaN points for the linear regression to be considered plausible |
> | Trend Threshold | `trend_threshold`{python exec} | `site_data.trend`{python exec} | Threshold for trend value to be considered significant |
> | Noise Threshold | `noise_threshold`{python exec} | `site_data.rmse`{python exec} | Threshold for noise, based on the RMSE and trend value |
> | Weak Trend Threshold | `weak_trend_threshold`{python exec} | `abs(site_data.trend)`{python exec} | Threshold for weak trend, in m/year |
> | Strong Trend Threshold | `strong_trend_threshold`{python exec} | `abs(site_data.trend)`{python exec} | Threshold for strong trend, in m/year |

