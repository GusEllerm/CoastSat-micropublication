from rocrate.rocrate import ROCrate
from rocrate.model.contextentity import ContextEntity
import tempfile
import shutil
import subprocess
import argparse
from pathlib import Path
import requests
import json
import os

def prepare_temp_directory(template_path):
    temp_dir = tempfile.TemporaryDirectory()
    temp_dir_path = Path(temp_dir.name)

    shutil.copy(template_path, temp_dir_path / "micropublication.smd")
    
    # Write the transect dict to data.json in the temp directory
    data_json_path = temp_dir_path / "data.json"
    with open(data_json_path, "w", encoding="utf-8") as f:
        json.dump(transect, f, ensure_ascii=False, indent=2)

    # Write the coordinates to coordinates.json in the temp directory
    coordinates_json_path = temp_dir_path / "coordinates.json"
    with open(coordinates_json_path, "w", encoding="utf-8") as f:
        json.dump(coordinates, f, ensure_ascii=False, indent=2)

    # Add top-level ro-crate-metadata.json
    crate_root = Path(__file__).parent
    top_level_manifest = crate_root / "ro-crate-metadata.json"
    if top_level_manifest.exists():
        shutil.copy(top_level_manifest, temp_dir_path / "ro-crate-metadata.json")

    # Recursively find and copy all nested ro-crate-metadata.json files
    for dirpath, dirnames, filenames in os.walk(crate_root):
        if "ro-crate-metadata.json" in filenames:
            full_manifest_path = Path(dirpath) / "ro-crate-metadata.json"
            relative_manifest_path = full_manifest_path.relative_to(crate_root)
            target_manifest_path = temp_dir_path / relative_manifest_path

            # Ensure parent directories exist
            target_manifest_path.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy(full_manifest_path, target_manifest_path)

    return temp_dir, temp_dir_path

def evaluate_micropublication(temp_dir_path):

    smd_files = list(temp_dir_path.glob("*.smd"))
    if not smd_files:
        raise FileNotFoundError("No .smd template file found in temporary directory")
    template = smd_files[0]

    print(template)

    data_json = temp_dir_path / "data.json"
    if not data_json.exists():
        raise FileNotFoundError(f"data.json not found in {temp_dir_path}")
    
    # Run the stencila pipeline to gen the micropub
    try:
        print("🧪 Running Stencila pipeline...")
        # Ensure pandas is installed in the subprocess environment
        subprocess.run(
            ["python", "-m", "pip", "install", "pandas"],
            cwd=temp_dir_path,
            check=True
        )
        subprocess.run(["stencila", "convert", template, f"{temp_dir_path}/DNF.json"], check=True)
        subprocess.run(["stencila", "render", f"{temp_dir_path}/DNF.json", f"{temp_dir_path}/DNF_eval.json", "--force-all", "--pretty"], check=True)
        subprocess.run(["stencila", "convert", f"{temp_dir_path}/DNF_eval.json", f"{temp_dir_path}/micropublication.html", "--pretty"], check=True)
        final_path = f"{temp_dir_path}/micropublication.html"
        return final_path if os.path.exists(final_path) else None
    except subprocess.CalledProcessError as e:
        print(f"❌ Error in Stencila pipeline")
        return None

def get_template_path(crate_path):
    """
    Loads the RO-Crate manifest and locates the template file based on type.
    """
    crate = ROCrate(crate_path)
    for entity in crate.get_entities():

        entity_type = entity.properties().get("@type", [])
        if isinstance(entity_type, str):
            entity_type = [entity_type]
        if all(t in entity_type for t in ["File", "SoftwareSourceCode", "SoftwareApplication"]):
            return Path(crate_path) / entity.id
    raise FileNotFoundError("Template with specified type not found in publication.crate")

def get_data_path(crate_path):

    pub_crate = ROCrate(crate_path)

    interface_entity = next(
        (e for e in pub_crate.get_entities()
         if set(e.properties().get("@type", [])) >= {"RO-Crate", "Dataset"}),
        None
    )
    if not interface_entity:
        raise FileNotFoundError("Interface crate not found in publication.crate")

    interface_crate_path = Path(crate_path) / interface_entity.id
    if not interface_crate_path.exists():
        raise FileNotFoundError(f"interface.crate directory not found at {interface_crate_path}")

    interface_crate = ROCrate(interface_crate_path)

    for entity in interface_crate.get_entities():
        example_of_work = entity.properties().get("exampleOfWork", [])
        for item in example_of_work:
            # #fp-transects_extended_geojson is the formal param representing 
            # the final result file generated by make_xlsx in the Coastsat workflow. 
            if isinstance(item, dict) and item.get("@id") == "#fp-transects_extended_geojson":
                return entity.id

    raise FileNotFoundError("No data entity found with exampleOfWork = #fp-transects_extended_geojson")

def download_data_file(data_url):
    """
    Downloads a file from the given URL to a temp directory under Path(__file__).parent / 'data'.
    Supports direct file URLs and GitHub blob URLs (converts to raw).
    Returns the local file path.
    Only downloads if the file does not already exist.
    """
    data_dir = Path(__file__).parent / "data"
    data_dir.mkdir(parents=True, exist_ok=True)

    # Convert GitHub blob URL to raw URL if needed
    if data_url.startswith("https://github.com/") and "/blob/" in data_url:
        data_url = data_url.replace("github.com", "raw.githubusercontent.com").replace("/blob/", "/")

    local_filename = data_url.split("/")[-1]
    local_path = data_dir / local_filename

    if local_path.exists():
        print(f"File already exists: {local_path}")
        return local_path

    with requests.get(data_url, stream=True) as r:
        r.raise_for_status()
        with open(local_path, "wb") as f:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
    return local_path

def get_transect_by_id(geojson_path, transect_id):
    """
    Loads a GeoJSON file and returns the feature whose properties['id'] matches transect_id.
    Uses a generator expression for faster lookup.
    """
    with open(geojson_path, "r", encoding="utf-8") as f:
        geojson = json.load(f)
    feature = next(
        (feat for feat in geojson.get("features", [])
         if feat.get("properties", {}).get("id") == transect_id),
        None
    )
    if feature is None:
        raise ValueError(f"Transect with id '{transect_id}' not found in {geojson_path}")
    return feature

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Generate a micropublication for a given transect ID.")
    parser.add_argument("transect_id", help="The transect ID to generate the micropublication for.")
    parser.add_argument("--output", help="Output file path for the generated micropublication HTML.", default="micropublication.html")
    args = parser.parse_args()

    template_path = get_template_path(Path(__file__).parent)
    data_path = get_data_path(Path(__file__).parent)

    data = download_data_file(data_path)
    transect_feature = get_transect_by_id(data, args.transect_id)
    transect = transect_feature.get("properties", {})
    coordinates = transect_feature.get("geometry")


    temp_dir_obj, temp_dir_path = prepare_temp_directory(template_path)
    micropub_path = evaluate_micropublication(temp_dir_path)
    if micropub_path:
        output_path = args.output if hasattr(args, "output") else "micropublication.html"
        shutil.copy(micropub_path, output_path)
        print(f"Micropublication written to {output_path}")
    else:
        print("Failed to generate micropublication.")

    temp_dir_obj.cleanup()  # Clean up the temporary directory
    print("Temporary directory cleaned up.")
